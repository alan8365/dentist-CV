{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import timm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from utils.core import main\n",
    "\n",
    "load_dotenv()\n",
    "matplotlib.use('module://matplotlib_inline.backend_inline')\n",
    "\n",
    "# %matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "data_dir = '..' / Path(os.getenv('DATASET_DIR'))\n",
    "yolo_model_dir = Path(os.getenv('YOLO_MODEL_DIR'))\n",
    "yolo_dir = yolo_model_dir / '..'\n",
    "model = YOLO(yolo_model_dir / 'enumerate.pt')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "SwinTransformer(\n  (patch_embed): PatchEmbed(\n    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n  )\n  (pos_drop): Dropout(p=0.0, inplace=False)\n  (layers): Sequential(\n    (0): BasicLayer(\n      (blocks): Sequential(\n        (0): SwinTransformerBlock(\n          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=128, out_features=384, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=128, out_features=128, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): Identity()\n          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=128, out_features=512, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=512, out_features=128, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (1): SwinTransformerBlock(\n          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=128, out_features=384, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=128, out_features=128, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.004)\n          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=128, out_features=512, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=512, out_features=128, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (downsample): PatchMerging(\n        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (reduction): Linear(in_features=512, out_features=256, bias=False)\n      )\n    )\n    (1): BasicLayer(\n      (blocks): Sequential(\n        (0): SwinTransformerBlock(\n          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=256, out_features=768, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=256, out_features=256, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.009)\n          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (1): SwinTransformerBlock(\n          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=256, out_features=768, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=256, out_features=256, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.013)\n          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (downsample): PatchMerging(\n        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (reduction): Linear(in_features=1024, out_features=512, bias=False)\n      )\n    )\n    (2): BasicLayer(\n      (blocks): Sequential(\n        (0): SwinTransformerBlock(\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.017)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (1): SwinTransformerBlock(\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.022)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (2): SwinTransformerBlock(\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.026)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (3): SwinTransformerBlock(\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.030)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (4): SwinTransformerBlock(\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.035)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (5): SwinTransformerBlock(\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.039)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (6): SwinTransformerBlock(\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.043)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (7): SwinTransformerBlock(\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.048)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (8): SwinTransformerBlock(\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.052)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (9): SwinTransformerBlock(\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.057)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (10): SwinTransformerBlock(\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.061)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (11): SwinTransformerBlock(\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.065)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (12): SwinTransformerBlock(\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.070)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (13): SwinTransformerBlock(\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.074)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (14): SwinTransformerBlock(\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.078)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (15): SwinTransformerBlock(\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.083)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (16): SwinTransformerBlock(\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.087)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (17): SwinTransformerBlock(\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.091)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (downsample): PatchMerging(\n        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n        (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n      )\n    )\n    (3): BasicLayer(\n      (blocks): Sequential(\n        (0): SwinTransformerBlock(\n          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.096)\n          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (1): SwinTransformerBlock(\n          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path): DropPath(drop_prob=0.100)\n          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  (head): Linear(in_features=1024, out_features=6, bias=True)\n)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = '.' / Path(os.getenv('ViT_MODEL_DIR'))\n",
    "model_path = model_dir / 'yolov8-base.pt'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vit_model = timm.create_model('swin_base_patch4_window7_224_in22k', num_classes=6)\n",
    "vit_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "vit_model.to(device)\n",
    "vit_model.eval()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "        filename     R.R             caries                             crown  \\\n0   00006145.jpg                         25                                     \n1   00008026.jpg                         46              11,12,21,22,26,32,36   \n2   00008075.jpg                                                                \n3   00008113.jpg                11,12,41,35                                36   \n4   00008117.jpg                   16,24,26                                     \n5   00008131.jpg                   15,17,34                                     \n6   00008136.jpg                   16,17,44                             32,33   \n7   00008137.jpg                      22,26                             36,37   \n8   00008140.jpg                16,21,45,47                                     \n9   00008145.jpg                   27,34,45                                46   \n11  00008154.jpg  37, 42                     14,16,17,24,25,26,27,33,34,35,38   \n13  00008156.jpg                   22,42,43                          24,25,32   \n14  00008157.jpg                      12,33        11,15,16,21,22,24,25,35,36   \n15  00008163.jpg                                                                \n16  00008166.jpg          11,21,23,26,45,46                                     \n17  00008169.jpg                         35                                     \n18  00008174.jpg                                                                \n19  00008177.jpg                      36,47                                     \n20  00008180.jpg                                                           36   \n21  00008181.jpg                         24                       14,22,26,34   \n22  00008186.jpg                                                                \n23  00008187.jpg      17              13,48                                     \n24  00008192.jpg                                                                \n25  00008197.jpg      35                 16                                     \n26  00008199.jpg                                               11,12,21,22,23   \n27  00008200.jpg                      34,46                                     \n29  00008210.jpg                                                           21   \n30  00008217.jpg      47                 37                                     \n32  00008223.jpg                                                                \n33  00008225.jpg                                                                \n\n                             endo  ...  \\\n0                                  ...   \n1                        11,21,36  ...   \n2                                  ...   \n3                              36  ...   \n4                                  ...   \n5                              44  ...   \n6                                  ...   \n7                                  ...   \n8                           11,21  ...   \n9                                  ...   \n11  17,26,31,33,34,38,41,42,43,44  ...   \n13                       37,45,47  ...   \n14                             15  ...   \n15                             36  ...   \n16                                 ...   \n17                                 ...   \n18                                 ...   \n19                                 ...   \n20                                 ...   \n21                       14,16,36  ...   \n22                                 ...   \n23                                 ...   \n24                                 ...   \n25                                 ...   \n26                    11,12,21,22  ...   \n27                                 ...   \n29                             21  ...   \n30                                 ...   \n32                                 ...   \n33                             17  ...   \n\n                                   filling             Imp     embedded  \\\n0                                       24                                \n1                                       17                           38   \n2                                    36,37                                \n3   14,15,16,17,22,24,25,26,27,37,38,46,47                                \n4                                 27,46,47                        38,48   \n5                                                                         \n6                                       28                                \n7                     27,31,34,41,44,45,47                                \n8                           16,21,25,27,28                           18   \n9                              16,26,27,47              46                \n11                                   46,47                                \n13                                                   21,27                \n14                       13,14,26,44,45,46                                \n15                    16,24,25,36,37,46,47                                \n16                                11,21,26                        28,38   \n17                                   36,47                                \n18     14,15,16,17,24,25,26,27,36,37,46,47                  18,28,38,48   \n19                                36,38,48                                \n20                             25,45,46,47                                \n21                          16,36,37,46,47                                \n22                                   35,37                                \n23                                      37                                \n24                                                                   38   \n25                                                          18,28,38,48   \n26                                27,44,45  25,33,44,45,47                \n27                                      46                                \n29                             12,26,37,47                                \n30                                                                18,28   \n32                                15,36,46                                \n33                                17,36,47                                \n\n       impacted                     missing  \n0                                            \n1                                            \n2                                            \n3                                            \n4                                            \n5                               36,37,46,47  \n6                      34,35,36,37,45,46,47  \n7                                            \n8                                  36,37,46  \n9                                            \n11                                 36,37,42  \n13               16,17,26,27,34,35,36,44,46  \n14                                           \n15           47                              \n16                                 16,17,36  \n17                                       46  \n18                                           \n19                                           \n20                                           \n21                                           \n22                                 25,37,46  \n23                        15,16,17,24,35,36  \n24                                           \n25                                       35  \n26                              14,15,36,37  \n27                                           \n29                                    36,46  \n30                  11,15,16,17,24,26,27,47  \n32                                 17,37,47  \n33  18,28,38,48                       26,46  \n\n[30 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>R.R</th>\n      <th>caries</th>\n      <th>crown</th>\n      <th>endo</th>\n      <th>...</th>\n      <th>filling</th>\n      <th>Imp</th>\n      <th>embedded</th>\n      <th>impacted</th>\n      <th>missing</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00006145.jpg</td>\n      <td></td>\n      <td>25</td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>24</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00008026.jpg</td>\n      <td></td>\n      <td>46</td>\n      <td>11,12,21,22,26,32,36</td>\n      <td>11,21,36</td>\n      <td>...</td>\n      <td>17</td>\n      <td></td>\n      <td>38</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00008075.jpg</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>36,37</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00008113.jpg</td>\n      <td></td>\n      <td>11,12,41,35</td>\n      <td>36</td>\n      <td>36</td>\n      <td>...</td>\n      <td>14,15,16,17,22,24,25,26,27,37,38,46,47</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00008117.jpg</td>\n      <td></td>\n      <td>16,24,26</td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>27,46,47</td>\n      <td></td>\n      <td>38,48</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>00008131.jpg</td>\n      <td></td>\n      <td>15,17,34</td>\n      <td></td>\n      <td>44</td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>36,37,46,47</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>00008136.jpg</td>\n      <td></td>\n      <td>16,17,44</td>\n      <td>32,33</td>\n      <td></td>\n      <td>...</td>\n      <td>28</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>34,35,36,37,45,46,47</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>00008137.jpg</td>\n      <td></td>\n      <td>22,26</td>\n      <td>36,37</td>\n      <td></td>\n      <td>...</td>\n      <td>27,31,34,41,44,45,47</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>00008140.jpg</td>\n      <td></td>\n      <td>16,21,45,47</td>\n      <td></td>\n      <td>11,21</td>\n      <td>...</td>\n      <td>16,21,25,27,28</td>\n      <td></td>\n      <td>18</td>\n      <td></td>\n      <td>36,37,46</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>00008145.jpg</td>\n      <td></td>\n      <td>27,34,45</td>\n      <td>46</td>\n      <td></td>\n      <td>...</td>\n      <td>16,26,27,47</td>\n      <td>46</td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>00008154.jpg</td>\n      <td>37, 42</td>\n      <td></td>\n      <td>14,16,17,24,25,26,27,33,34,35,38</td>\n      <td>17,26,31,33,34,38,41,42,43,44</td>\n      <td>...</td>\n      <td>46,47</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>36,37,42</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>00008156.jpg</td>\n      <td></td>\n      <td>22,42,43</td>\n      <td>24,25,32</td>\n      <td>37,45,47</td>\n      <td>...</td>\n      <td></td>\n      <td>21,27</td>\n      <td></td>\n      <td></td>\n      <td>16,17,26,27,34,35,36,44,46</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>00008157.jpg</td>\n      <td></td>\n      <td>12,33</td>\n      <td>11,15,16,21,22,24,25,35,36</td>\n      <td>15</td>\n      <td>...</td>\n      <td>13,14,26,44,45,46</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>00008163.jpg</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>36</td>\n      <td>...</td>\n      <td>16,24,25,36,37,46,47</td>\n      <td></td>\n      <td></td>\n      <td>47</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>00008166.jpg</td>\n      <td></td>\n      <td>11,21,23,26,45,46</td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>11,21,26</td>\n      <td></td>\n      <td>28,38</td>\n      <td></td>\n      <td>16,17,36</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>00008169.jpg</td>\n      <td></td>\n      <td>35</td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>36,47</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>00008174.jpg</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>14,15,16,17,24,25,26,27,36,37,46,47</td>\n      <td></td>\n      <td>18,28,38,48</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>00008177.jpg</td>\n      <td></td>\n      <td>36,47</td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>36,38,48</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>00008180.jpg</td>\n      <td></td>\n      <td></td>\n      <td>36</td>\n      <td></td>\n      <td>...</td>\n      <td>25,45,46,47</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>00008181.jpg</td>\n      <td></td>\n      <td>24</td>\n      <td>14,22,26,34</td>\n      <td>14,16,36</td>\n      <td>...</td>\n      <td>16,36,37,46,47</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>00008186.jpg</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>35,37</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>25,37,46</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>00008187.jpg</td>\n      <td>17</td>\n      <td>13,48</td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>37</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>15,16,17,24,35,36</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>00008192.jpg</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td>38</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>00008197.jpg</td>\n      <td>35</td>\n      <td>16</td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td>18,28,38,48</td>\n      <td></td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>00008199.jpg</td>\n      <td></td>\n      <td></td>\n      <td>11,12,21,22,23</td>\n      <td>11,12,21,22</td>\n      <td>...</td>\n      <td>27,44,45</td>\n      <td>25,33,44,45,47</td>\n      <td></td>\n      <td></td>\n      <td>14,15,36,37</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>00008200.jpg</td>\n      <td></td>\n      <td>34,46</td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>46</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>00008210.jpg</td>\n      <td></td>\n      <td></td>\n      <td>21</td>\n      <td>21</td>\n      <td>...</td>\n      <td>12,26,37,47</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>36,46</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>00008217.jpg</td>\n      <td>47</td>\n      <td>37</td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td>18,28</td>\n      <td></td>\n      <td>11,15,16,17,24,26,27,47</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>00008223.jpg</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>15,36,46</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>17,37,47</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>00008225.jpg</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>17</td>\n      <td>...</td>\n      <td>17,36,47</td>\n      <td></td>\n      <td></td>\n      <td>18,28,38,48</td>\n      <td>26,46</td>\n    </tr>\n  </tbody>\n</table>\n<p>30 rows Ã— 11 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data = data_dir / 'pano-report.csv'\n",
    "\n",
    "df = pd.read_csv(csv_data)\n",
    "df = df.replace('0', '')\n",
    "df = df.replace(0.0, '')\n",
    "df = df.drop('comment', axis=1)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df = df[:32]\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "df_columns = df.columns[1:]\n",
    "x_for_possible = [(col_name,) for col_name in df_columns]\n",
    "\n",
    "mlb.fit(x_for_possible)\n",
    "mlb.transform([['R.R', 'missing'], ['Imp'], []])\n",
    "# enc.transform(x_for_possible)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "[array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n array([[0, 0, 0, 1, 0, 1, 0, 0, 0, 1],\n        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0, 1, 0, 0, 0, 1],\n        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0, 1, 0, 0, 0, 1],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teeth_number_possible = [[i * 10 + j for j in range(1, 9)] for i in range(1, 5)]\n",
    "teeth_number_possible = np.hstack(teeth_number_possible).tolist()\n",
    "# teeth_number_possible = teeth_number_possible.reshape(-1, 1)\n",
    "\n",
    "y_encode = []\n",
    "tooth_idx_dict = {tooth_number: idx for idx, tooth_number in enumerate(teeth_number_possible)}\n",
    "for index, row in df.iterrows():\n",
    "    temp = [[] for _ in teeth_number_possible]\n",
    "\n",
    "    for col_name in df_columns:\n",
    "        class_tooth = row[col_name]\n",
    "        class_tooth = class_tooth.split(',') if class_tooth else []\n",
    "        class_tooth = list(map(int, class_tooth))\n",
    "\n",
    "        for tooth_number in class_tooth:\n",
    "            idx = tooth_idx_dict[tooth_number]\n",
    "            temp[idx].append(col_name)\n",
    "\n",
    "    y_encode.append(mlb.transform(temp))\n",
    "\n",
    "y_encode[:2]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x640 1 11, 1 12, 2 13s, 2 14s, 2 15s, 2 16s, 1 17, 1 21, 1 22, 1 23, 1 26, 1 28, 1 31, 1 32, 1 33, 1 34, 1 35, 1 37, 1 41, 1 42, 1 43, 2 44s, 2 45s, 1 47, 2 48s, 1: 320x640 1 11, 1 12, 1 13, 1 14, 1 15, 1 16, 1 18, 1 21, 1 22, 1 23, 1 24, 1 25, 1 26, 1 27, 1 28, 1 31, 1 32, 1 33, 1 34, 1 35, 1 36, 1 37, 1 38, 1 41, 1 42, 1 43, 1 44, 1 45, 1 46, 1 47, 1 48, 2: 320x640 1 11, 1 12, 1 13, 1 14, 2 15s, 1 16, 1 17, 1 21, 1 22, 1 23, 1 24, 1 25, 1 26, 1 27, 1 28, 1 31, 1 32, 1 33, 1 34, 1 35, 1 36, 1 37, 1 41, 1 42, 1 43, 1 44, 1 45, 1 46, 2 47s, 3: 320x640 1 11, 1 12, 1 13, 1 14, 1 15, 1 16, 1 17, 1 22, 2 23s, 1 24, 1 25, 1 26, 1 27, 1 31, 1 32, 1 33, 1 34, 1 35, 1 36, 1 37, 1 38, 1 41, 1 42, 1 43, 1 44, 2 45s, 1 46, 1 47, 4: 320x640 1 11, 1 12, 1 13, 1 14, 1 15, 1 16, 1 17, 1 21, 1 22, 1 23, 1 24, 1 25, 1 26, 1 27, 1 31, 1 32, 1 33, 1 34, 1 35, 1 36, 1 37, 1 41, 1 42, 1 43, 1 44, 1 45, 1 46, 1 47, 5: 320x640 1 11, 1 12, 1 13, 1 14, 1 15, 1 16, 1 17, 1 21, 1 22, 1 23, 1 24, 1 25, 1 26, 1 27, 1 31, 1 32, 1 33, 1 34, 1 35, 1 37, 1 41, 1 42, 1 43, 2 44s, 2 45s, 6: 320x640 1 11, 1 12, 1 13, 1 16, 2 17s, 1 21, 1 22, 1 23, 1 24, 1 25, 1 26, 2 27s, 1 28, 1 31, 2 33s, 2 41s, 1 42, 2 43s, 7: 320x640 1 11, 1 12, 1 13, 1 14, 1 15, 1 16, 1 17, 1 21, 1 22, 1 23, 1 24, 1 25, 1 26, 1 27, 1 31, 1 32, 1 33, 1 34, 1 35, 1 36, 1 37, 1 41, 1 42, 1 43, 1 44, 1 45, 1 46, 1 47, 8: 320x640 1 11, 1 12, 1 13, 1 14, 1 15, 1 16, 1 17, 1 18, 1 21, 1 22, 1 23, 1 24, 1 25, 1 27, 1 28, 1 31, 1 32, 1 33, 1 34, 1 35, 1 41, 1 42, 1 43, 1 44, 1 45, 1 47, 1 48, 9: 320x640 1 11, 1 12, 1 13, 1 14, 1 15, 1 16, 1 17, 1 21, 1 22, 1 23, 1 24, 1 25, 1 26, 1 27, 1 31, 1 32, 1 33, 1 34, 1 35, 1 36, 1 37, 1 41, 1 42, 1 43, 1 44, 1 45, 1 46, 1 47, 10: 320x640 2 11s, 1 12, 1 13, 2 14s, 1 15, 1 16, 2 17s, 1 21, 1 22, 1 27, 1 31, 1 32, 1 33, 1 42, 1 43, 2 44s, 2 45s, 1 46, 1 47, 1 48, 11: 320x640 1 11, 1 12, 2 13s, 2 14s, 1 15, 2 21s, 1 22, 1 23, 1 24, 1 31, 1 32, 1 33, 1 37, 1 41, 1 42, 1 43, 1 45, 1 47, 12: 320x640 1 11, 1 12, 1 13, 1 14, 1 15, 1 16, 1 17, 1 21, 2 22s, 1 23, 1 24, 1 25, 1 26, 1 27, 1 28, 1 31, 1 32, 1 33, 1 34, 1 35, 1 36, 1 37, 1 38, 1 41, 1 42, 1 43, 1 44, 1 45, 1 46, 1 47, 13: 320x640 1 11, 1 12, 1 13, 1 15, 1 16, 1 17, 1 21, 1 22, 1 23, 1 25, 1 26, 1 27, 2 31s, 2 32s, 2 33s, 1 34, 1 35, 1 36, 1 37, 1 41, 1 42, 2 43s, 1 44, 1 45, 1 47, 1 48, 14: 320x640 1 11, 1 12, 1 13, 1 14, 1 15, 1 21, 1 22, 1 23, 1 24, 1 25, 1 26, 1 27, 1 28, 1 31, 1 32, 1 33, 1 34, 1 35, 1 37, 1 38, 1 41, 1 42, 1 43, 1 44, 1 45, 1 46, 1 47, 1 48, 15: 320x640 2 11s, 1 12, 1 13, 2 14s, 2 15s, 2 16s, 2 17s, 2 18s, 1 21, 1 22, 1 23, 1 24, 1 25, 1 27, 1 31, 1 32, 1 33, 1 34, 1 37, 1 38, 1 41, 1 42, 1 43, 1 44, 2 45s, 1 46, 2 47s, 1 48, 16: 320x640 1 11, 1 12, 1 13, 1 14, 1 15, 1 16, 1 17, 1 18, 1 21, 1 22, 2 23s, 1 24, 1 25, 1 26, 1 27, 1 28, 2 31s, 1 32, 1 33, 1 34, 1 35, 1 36, 1 37, 1 38, 1 41, 1 42, 1 43, 1 44, 1 45, 1 46, 1 47, 17: 320x640 1 11, 1 12, 1 13, 1 14, 1 15, 1 16, 1 18, 1 21, 1 22, 1 23, 1 24, 1 25, 1 26, 1 27, 1 31, 2 32s, 1 33, 1 34, 1 35, 1 36, 1 37, 1 38, 1 41, 1 42, 1 43, 1 44, 1 45, 1 46, 1 47, 1 48, 18: 320x640 1 11, 1 12, 1 13, 1 14, 1 15, 1 16, 1 17, 1 21, 1 22, 1 23, 1 24, 1 25, 1 26, 1 27, 1 31, 1 32, 1 33, 1 34, 1 35, 1 36, 1 37, 2 41s, 1 42, 1 43, 1 44, 1 45, 1 46, 1 47, 19: 320x640 2 11s, 2 12s, 2 13s, 1 14, 1 15, 1 16, 1 17, 1 18, 2 21s, 2 22s, 1 23, 1 24, 1 25, 1 26, 1 27, 1 31, 1 32, 1 33, 1 34, 1 35, 1 36, 1 37, 1 41, 1 42, 1 43, 1 44, 1 45, 1 47, 1 48, 20: 320x640 1 11, 1 12, 1 13, 1 14, 1 15, 1 16, 1 17, 1 18, 1 21, 1 22, 1 23, 1 24, 1 26, 1 27, 1 28, 1 31, 1 32, 2 33s, 1 34, 1 35, 1 37, 1 38, 1 41, 1 42, 1 43, 1 44, 3 45s, 1 46, 2 47s, 21: 320x640 1 11, 1 12, 1 13, 1 14, 1 18, 1 21, 1 22, 1 23, 1 25, 1 26, 2 27s, 1 31, 1 32, 1 33, 1 34, 1 37, 1 41, 1 42, 1 43, 1 44, 1 45, 1 46, 1 47, 1 48, 22: 320x640 1 11, 2 12s, 1 13, 3 14s, 2 15s, 2 16s, 1 17, 1 21, 2 22s, 1 23, 1 25, 1 26, 1 27, 1 31, 1 32, 1 33, 1 34, 1 35, 1 36, 1 37, 1 38, 1 41, 1 42, 1 43, 1 44, 1 45, 1 46, 1 47, 23: 320x640 1 11, 1 12, 1 13, 1 14, 1 15, 1 16, 1 17, 1 18, 1 21, 1 22, 1 23, 1 24, 2 25s, 1 26, 1 27, 1 28, 1 31, 1 32, 1 33, 1 34, 1 35, 1 36, 1 37, 1 38, 1 41, 1 42, 1 43, 1 44, 1 45, 1 46, 1 47, 1 48, 24: 320x640 5 11s, 2 12s, 1 13, 2 16s, 2 17s, 1 25, 1 31, 1 32, 1 33, 1 34, 1 41, 2 42s, 2 43s, 2 44s, 2 45s, 1 46, 1 47, 1 48, 25: 320x640 1 11, 1 12, 1 13, 1 14, 1 15, 1 16, 1 17, 1 21, 1 22, 1 23, 1 24, 1 25, 1 26, 1 27, 1 31, 1 32, 1 33, 1 34, 1 35, 1 36, 1 37, 1 38, 1 41, 1 42, 1 43, 1 44, 1 45, 1 46, 1 47, 26: 320x640 2 11s, 1 12, 2 13s, 1 14, 2 15s, 1 16, 1 17, 1 21, 1 22, 1 27, 1 31, 1 32, 1 33, 1 34, 1 35, 2 37s, 1 38, 1 41, 1 42, 1 43, 1 44, 2 45s, 1 47, 1 48, 27: 320x640 2 11s, 1 12, 2 13s, 1 21, 1 22, 1 23, 1 25, 1 28, 2 31s, 1 32, 1 33, 1 34, 1 35, 1 36, 1 37, 1 41, 1 42, 1 43, 1 44, 1 45, 1 46, 28: 320x640 1 11, 1 12, 1 13, 1 14, 1 15, 1 16, 1 21, 1 22, 1 23, 2 24s, 2 25s, 1 26, 1 27, 1 31, 1 32, 1 33, 1 34, 1 35, 1 36, 1 37, 1 41, 1 42, 1 43, 1 44, 1 45, 1 46, 29: 320x640 1 11, 1 12, 1 13, 1 14, 1 15, 1 16, 1 17, 1 18, 1 21, 1 22, 1 23, 1 24, 1 25, 2 27s, 1 28, 1 31, 1 32, 1 33, 1 34, 1 35, 1 36, 1 37, 1 38, 1 41, 1 42, 1 43, 1 44, 1 45, 1 47, 1 48, 1124.0ms\n",
      "Speed: 1.6ms preprocess, 37.5ms inference, 0.3ms postprocess per image at shape (1, 3, 320, 640)\n"
     ]
    }
   ],
   "source": [
    "image_names = df['filename']\n",
    "image_names = image_names.apply(lambda s: data_dir / 'phase-2' / s)\n",
    "image_names = image_names.tolist()\n",
    "\n",
    "results = model(image_names)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "anomaly_dict = {}\n",
    "\n",
    "for result in results:\n",
    "    org_img = result.orig_img\n",
    "    for xyxy in result.boxes.xyxy:\n",
    "        x1, y1, x2, y2 = xyxy.int()\n",
    "        tooth_img = org_img[y1:y2, x1:x2]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from utils.vit import predict\n",
    "\n",
    "a = predict()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 1, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 1, 0, 0]])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(a)\n",
    "np.vstack(y_encode)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31m\u001B[1mrequirements:\u001B[0m /opt/miniconda3/envs/pt113/lib/python3.9/site-packages/yolov5/requirements.txt not found, check failed.\n",
      "\u001B[31m\u001B[1mrequirements:\u001B[0m /opt/miniconda3/envs/pt113/lib/python3.9/site-packages/yolov5/requirements.txt not found, check failed.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "[Errno 2] No such file or directory: '/Users/lucyxu/PycharmProjects/dentist-CV/YOLO/weights/anomaly.pt'. Cache may be out of date, try `force_reload=True` or see https://github.com/ultralytics/yolov5/issues/36 for help.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m~/PycharmProjects/dentist-CV/YOLO/hubconf.py\u001B[0m in \u001B[0;36m_create\u001B[0;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001B[0m\n\u001B[1;32m     45\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mpretrained\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mchannels\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m3\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mclasses\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m80\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 46\u001B[0;31m             \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDetectMultiBackend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# download/load FP32 model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     47\u001B[0m             \u001B[0;31m# model = models.experimental.attempt_load(path, map_location=device)  # download/load FP32 model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/miniconda3/envs/pt113/lib/python3.9/site-packages/yolov5/models/common.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, weights, device, dnn, data, fp16)\u001B[0m\n\u001B[1;32m    309\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mpt\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# PyTorch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 310\u001B[0;31m             \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mattempt_load\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mweights\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mweights\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mw\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmap_location\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    311\u001B[0m             \u001B[0mstride\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstride\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m32\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# model stride\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/miniconda3/envs/pt113/lib/python3.9/site-packages/yolov5/models/experimental.py\u001B[0m in \u001B[0;36mattempt_load\u001B[0;34m(weights, map_location, inplace, fuse)\u001B[0m\n\u001B[1;32m     98\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0myolov5_in_syspath\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 99\u001B[0;31m             \u001B[0mckpt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mattempt_download\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmap_location\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmap_location\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# load\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    100\u001B[0m         \u001B[0mckpt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mckpt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'ema'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mckpt\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'model'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# FP32 model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/miniconda3/envs/pt113/lib/python3.9/site-packages/torch/serialization.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001B[0m\n\u001B[1;32m    593\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 594\u001B[0;31m     \u001B[0;32mwith\u001B[0m \u001B[0m_open_file_like\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'rb'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mopened_file\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    595\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0m_is_zipfile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mopened_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/miniconda3/envs/pt113/lib/python3.9/site-packages/torch/serialization.py\u001B[0m in \u001B[0;36m_open_file_like\u001B[0;34m(name_or_buffer, mode)\u001B[0m\n\u001B[1;32m    229\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0m_is_path\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname_or_buffer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 230\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0m_open_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    231\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/miniconda3/envs/pt113/lib/python3.9/site-packages/torch/serialization.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, name, mode)\u001B[0m\n\u001B[1;32m    210\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 211\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_open_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    212\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Users/lucyxu/PycharmProjects/dentist-CV/YOLO/weights/anomaly.pt'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-22-eb7da21c1784>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0manomaly_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimage_names\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0miou_threshold\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0my_predict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mfilename\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'filename'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mteeth\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0manomaly_dict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/dentist-CV/utils/core.py\u001B[0m in \u001B[0;36mmain\u001B[0;34m(image_names, iou_threshold)\u001B[0m\n\u001B[1;32m     37\u001B[0m                                         \u001B[0mpath\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mYOLO_model_dir\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0;34m'8-bound.pt'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msource\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'local'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m                                         verbose=False)\n\u001B[0;32m---> 39\u001B[0;31m     anomaly_detect_model = torch.hub.load(str((YOLO_model_dir / '..').resolve()), 'custom',\n\u001B[0m\u001B[1;32m     40\u001B[0m                                           path=YOLO_model_dir / 'anomaly.pt', source='local')\n\u001B[1;32m     41\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/miniconda3/envs/pt113/lib/python3.9/site-packages/torch/hub.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(repo_or_dir, model, source, force_reload, verbose, skip_validation, *args, **kwargs)\u001B[0m\n\u001B[1;32m    397\u001B[0m         \u001B[0mrepo_or_dir\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_get_cache_or_reload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrepo_or_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mforce_reload\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mskip_validation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    398\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 399\u001B[0;31m     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_load_local\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrepo_or_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    400\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    401\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/miniconda3/envs/pt113/lib/python3.9/site-packages/torch/hub.py\u001B[0m in \u001B[0;36m_load_local\u001B[0;34m(hubconf_dir, model, *args, **kwargs)\u001B[0m\n\u001B[1;32m    426\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    427\u001B[0m     \u001B[0mentry\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_load_entry_from_hubconf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhub_module\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 428\u001B[0;31m     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mentry\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    429\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    430\u001B[0m     \u001B[0msys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mremove\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhubconf_dir\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/dentist-CV/YOLO/hubconf.py\u001B[0m in \u001B[0;36mcustom\u001B[0;34m(path, autoshape, verbose, device)\u001B[0m\n\u001B[1;32m     68\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mcustom\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'path/to/model.pt'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mautoshape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m     \u001B[0;31m# YOLOv5 custom or local model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 70\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_create\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mautoshape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mautoshape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mverbose\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     71\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/dentist-CV/YOLO/hubconf.py\u001B[0m in \u001B[0;36m_create\u001B[0;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001B[0m\n\u001B[1;32m     63\u001B[0m         \u001B[0mhelp_url\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'https://github.com/ultralytics/yolov5/issues/36'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     64\u001B[0m         \u001B[0ms\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mf'{e}. Cache may be out of date, try `force_reload=True` or see {help_url} for help.'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 65\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     66\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mException\u001B[0m: [Errno 2] No such file or directory: '/Users/lucyxu/PycharmProjects/dentist-CV/YOLO/weights/anomaly.pt'. Cache may be out of date, try `force_reload=True` or see https://github.com/ultralytics/yolov5/issues/36 for help."
     ]
    }
   ],
   "source": [
    "anomaly_dict = main(image_names, iou_threshold=0.3)\n",
    "y_predict = []\n",
    "\n",
    "for filename in df['filename']:\n",
    "    teeth = anomaly_dict[filename[:-4]]\n",
    "\n",
    "    temp = [[] for _ in teeth_number_possible]\n",
    "    for tooth_number, anomalies in teeth.items():\n",
    "        try:\n",
    "            idx = tooth_idx_dict[tooth_number]\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        temp[idx] = list(anomalies)\n",
    "\n",
    "    y_predict.append(mlb.transform(temp))\n",
    "\n",
    "y_predict[:2]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(y_predict)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_predict_sum = np.vstack([np.sum(i, axis=0) for i in y_predict])\n",
    "y_encode_sum = np.vstack([np.sum(i, axis=0) for i in y_encode])\n",
    "\n",
    "# np.r_[mlb.classes_.reshape((1, 10)), y_encode_sum - y_predict_sum]\n",
    "np.c_[np.r_[[0], df['filename'].to_numpy()], np.r_[mlb.classes_.reshape((1, 10)), y_encode_sum - y_predict_sum]]\n",
    "# mlb.inverse_transform(y_encode[0])\n",
    "# mlb.classes_.shape\n",
    "# (y_encode_sum - y_predict_sum).shape\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_encode_stack = np.vstack(y_encode)\n",
    "y_predict_stack = np.vstack(y_predict)\n",
    "\n",
    "acc = accuracy_score(y_encode_stack, y_predict_stack)\n",
    "precision = precision_score(y_encode_stack, y_predict_stack, average=None)\n",
    "recall = recall_score(y_encode_stack, y_predict_stack, average=None)\n",
    "f1 = f1_score(y_encode_stack, y_predict_stack, average=None)\n",
    "\n",
    "np.c_[mlb.classes_, precision, recall, f1]\n",
    "# acc\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
